{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection Parameters to establish Snowflake connection\n",
    "connection_parameters = {\n",
    "    \"user\":\"KRISHNAOV\",\n",
    "    \"password\":\"ovAIIntern@09/05\",\n",
    "    \"account\":\"kyb42073.us-east-1\",\n",
    "    \"warehouse\":\"OPENVIEW\",\n",
    "    \"database\":\"COMPANYRECSDB_TEST\",\n",
    "    \"schema\":\"PUBLIC\",\n",
    "    \"role\":\"ACCOUNTADMIN\"\n",
    "}\n",
    "\n",
    "# Establish a session with Snowflake\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the large CSV file and the local directory to store chunks\n",
    "file_path = '<path_to_csv_file>'\n",
    "local_dir = '<directory_to_store_chunks>'\n",
    "\n",
    "# Define the number of rows to use for the schema sample and the chunk size for reading the CSV\n",
    "sample_size = 1000\n",
    "chunk_size = 100000  # Size of each chunk to read i.e number of rows\n",
    "\n",
    "stage_name = '<stage_name'\n",
    "table_name = 'table_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a sample of the CSV file to infer schema\n",
    "sample_df = pd.read_csv(file_path, nrows=sample_size)\n",
    "sf_sample_df = session.create_dataframe(sample_df)\n",
    "\n",
    "# Create a table with the inferred schema if it doesn't exist\n",
    "table_exists_query = f\"SELECT COUNT(*) AS CNT FROM information_schema.tables WHERE table_schema = '{connection_parameters['schema']}' AND table_name = '{table_name}'\"\n",
    "table_exists_result = session.sql(table_exists_query).collect()\n",
    "\n",
    "# Create a table with the inferred schema if it doesn't exist\n",
    "if table_exists_result[0]['CNT'] == 0:\n",
    "    sf_sample_df.write.mode('overwrite').save_as_table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to skip is set to the sample size for the first chunk\n",
    "skiprows = sample_size\n",
    "\n",
    "# Check if the stage exists and create it if not\n",
    "create_stage_query = f\"CREATE STAGE IF NOT EXISTS {stage_name} FILE_FORMAT = (TYPE = 'CSV')\"\n",
    "session.sql(create_stage_query).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the CSV file in chunks, skipping the sample rows in the first chunk\n",
    "for i, chunk in enumerate(pd.read_csv(file_path, chunksize=chunk_size, skiprows=range(1, skiprows))):\n",
    "    # Update skiprows to None for subsequent chunks\n",
    "    skiprows = None\n",
    "    \n",
    "    # Define a unique file name for each chunk\n",
    "    file_name = f\"chunk_{i}.csv\"\n",
    "    local_file_path = os.path.join(local_dir, file_name)\n",
    "    \n",
    "    # Save the chunk locally\n",
    "    chunk.to_csv(local_file_path, index=False)\n",
    "    \n",
    "    # Upload the file to the stage with auto compression\n",
    "    put_command = f\"\"\"\n",
    "    PUT file://{local_file_path} @{stage_name} \n",
    "    AUTO_COMPRESS=TRUE\n",
    "    \"\"\"\n",
    "    session.sql(put_command).collect()\n",
    "    \n",
    "    # Copy the data into the Snowflake table\n",
    "    copy_command = f\"\"\"\n",
    "        COPY INTO {table_name}\n",
    "        FROM @{stage_name}\n",
    "        FILE_FORMAT = (TYPE = 'CSV' COMPRESSION = 'GZIP' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER = 1)\n",
    "        ON_ERROR = 'CONTINUE'\n",
    "    \"\"\"\n",
    "    session.sql(copy_command).collect()\n",
    "    \n",
    "    # Optionally, remove the local file after it's uploaded\n",
    "    os.remove(local_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the session\n",
    "session.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
